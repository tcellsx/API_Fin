{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47434634",
   "metadata": {},
   "source": [
    "# humming2music Demo Pipeline\n",
    "Linear notebook that stitches modules 01-09 together.\n",
    "\n",
    "Adjust the variables in each cell to run the full pipeline. Dependencies: pydub, librosa, numpy, sounddevice (optional for recording), ffmpeg for pydub/mp3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a11aa5",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport platform\n\nprint(\"Python exe:\", sys.executable)\nprint(\"Python version:\", platform.python_version())\n\nimport sys\nprint(\"Using interpreter:\", sys.executable)\n\n# Install packages in the current kernel's Python\nimport subprocess\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\",\n    \"librosa\", \"pydub\", \"soundfile\", \"sounddevice\",\n])"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da227ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa: 0.11.0\n",
      "pydub: 0.25.1\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata as md\n",
    "\n",
    "print(\"librosa:\", md.version(\"librosa\"))\n",
    "print(\"pydub:\", md.version(\"pydub\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfed8b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python exe: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.7.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
      "Torch version 2.9.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.7.0 is the most recent version that has been tested.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:tflite-runtime is not installed. If you plan to use a TFLite Model, reinstall basic-pitch with `pip install 'basic-pitch tflite-runtime'` or `pip install 'basic-pitch[tf]'\n",
      "WARNING:root:Tensorflow is not installed. If you plan to use a TF Saved Model, reinstall basic-pitch with `pip install 'basic-pitch[tf]'`\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/resampy/filters.py:50: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music\n",
      "Data/raw: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/raw\n",
      "Outputs/generated: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/generated\n"
     ]
    }
   ],
   "source": [
    "# 0. Setup paths and imports\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import librosa, pydub\n",
    "\n",
    "print(\"Python exe:\", sys.executable)\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import (\n",
    "    GLOBAL_AUDIO_CONFIG,\n",
    "    DEFAULT_PREPROCESSING_CONFIG,\n",
    "    DEFAULT_MELODY_EXTRACTION_CONFIG,\n",
    "    DEFAULT_MELODY_REPRESENTATION_CONFIG,\n",
    "    GLOBAL_STYLE_CONFIG,\n",
    "    DEFAULT_POSTPROCESSING_CONFIG,\n",
    "    DEFAULT_SIMILARITY_CONFIG,\n",
    "    RAW_AUDIO_DIR, PROCESSED_AUDIO_DIR, GENERATED_AUDIO_DIR, POSTPROCESSED_AUDIO_DIR, EVAL_OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "from src.audio_input import AudioInputManager\n",
    "from src.preprocessing import Preprocessor\n",
    "from src.melody_extraction import MelodyExtractor\n",
    "from src.melody_representation import MelodyRepresenter\n",
    "from src.style_and_model_config import StyleConfigManager\n",
    "from src.music_generation import MusicGenerator\n",
    "from src.postprocessing_export import Postprocessor\n",
    "from src.similarity_evaluation import SimilarityEvaluator\n",
    "\n",
    "for d in [RAW_AUDIO_DIR, PROCESSED_AUDIO_DIR, GENERATED_AUDIO_DIR, POSTPROCESSED_AUDIO_DIR, EVAL_OUTPUT_DIR]:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', PROJECT_ROOT)\n",
    "print('Data/raw:', RAW_AUDIO_DIR)\n",
    "print('Outputs/generated:', GENERATED_AUDIO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff89cff",
   "metadata": {},
   "outputs": [],
   "source": "# 1. Audio input (upload or record)\nfrom pathlib import Path\n\nSESSION_ID = \"demo_melody\"\n\n# Use test audio file\nUPLOAD_PATH = RAW_AUDIO_DIR / \"Lraw.m4a\"\n\naudio_manager = AudioInputManager()\naudio_meta = audio_manager.ingest_upload(UPLOAD_PATH, session_id=SESSION_ID)\n\naudio_meta_dict = audio_meta.to_dict()\naudio_meta_dict"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d523282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav',\n",
       " 'original_duration_sec': 19.722,\n",
       " 'processed_duration_sec': 18.415,\n",
       " 'sample_rate': 16000,\n",
       " 'applied_steps': ['trim_silence', 'highpass', 'normalize'],\n",
       " 'notes': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Preprocessing\n",
    "preprocessor = Preprocessor()\n",
    "pre_meta = preprocessor.preprocess(audio_meta.path)\n",
    "pre_meta_dict = pre_meta.to_dict()\n",
    "pre_meta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d112fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting MIDI for /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav...\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "isfinite: True\n",
      "shape: (1, 43844, 1)\n",
      "dtype: float32\n",
      "\n",
      "\n",
      "  Creating midi...\n",
      "  ðŸ’… Saved to /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine_basic_pitch.mid\n",
      "Generated MIDI: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine_basic_pitch.mid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'extractor': 'basic_pitch (onnx)',\n",
       " 'source_audio': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Melody extraction\n",
    "extractor = MelodyExtractor()\n",
    "contour = extractor.extract(pre_meta.path)\n",
    "contour_dict = contour.to_dict()\n",
    "print(f\"Generated MIDI: {contour_dict['midi_path']}\")\n",
    "contour_dict['metadata']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1c5016b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'notes': 52, 'tempo_bpm': 183.91386953768213}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Melody representation\n",
    "representer = MelodyRepresenter()\n",
    "\n",
    "rep = representer.represent(midi_path=contour.midi_path)\n",
    "rep_dict = rep.to_dict()\n",
    "\n",
    "rep_summary = {\n",
    "    'notes': len(rep_dict['note_sequence']),\n",
    "    'tempo_bpm': rep_dict['rhythm_profile'].get('estimated_tempo_bpm'),\n",
    "}\n",
    "rep_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8fc62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'ambient',\n",
       " 'description': 'Ethereal ambient soundscape with pads and evolving textures.',\n",
       " 'mood': 'calm',\n",
       " 'tempo_bpm': 70,\n",
       " 'instruments': ['pads', 'drones', 'textures'],\n",
       " 'model_configs': {'stub': {'model_name': 'stub',\n",
       "   'prompt': 'A calm ambient soundscape with airy pads and evolving textures.',\n",
       "   'max_duration_sec': 30}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Style selection\n",
    "style_manager = StyleConfigManager()\n",
    "available_styles = style_manager.list_styles()\n",
    "STYLE_NAME = available_styles[1]  # pick first by default\n",
    "style_config = style_manager.get_style(STYLE_NAME)\n",
    "style_config.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40e2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type musicgen_melody to instantiate a model of type musicgen. This is not supported for all configurations of models and can yield errors.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.58s/it]\n",
      "Some weights of MusicgenForConditionalGeneration were not initialized from the model checkpoint at facebook/musicgen-melody and are newly initialized: ['decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.24.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.24.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.25.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.25.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.26.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.26.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.27.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.27.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.28.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.28.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.29.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.29.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.30.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.30.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.31.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.31.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.32.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.32.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.33.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.33.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.34.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.34.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.35.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.35.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.36.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.36.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.37.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.37.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.38.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.38.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.39.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.39.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.40.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.40.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.41.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.41.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.42.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.42.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.43.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.43.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.44.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.44.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.45.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.45.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.46.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.46.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.47.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.47.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with prompt: Ethereal ambient soundscape with pads and evolving textures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'musicgen-transformers',\n",
       " 'style_name': 'ambient',\n",
       " 'audio_path': 'outputs/generated/gen_ambient_20251201_232336.wav',\n",
       " 'duration_sec': 10,\n",
       " 'generation_metadata': {'prompt': 'Ethereal ambient soundscape with pads and evolving textures.'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Music generation \n",
    "\n",
    "generator = MusicGenerator(model_size='melody', device='cpu') \n",
    "\n",
    "if hasattr(style_config, 'to_dict'):\n",
    "    style_data = style_config.to_dict()\n",
    "else:\n",
    "    style_data = style_config\n",
    "\n",
    "prompt = style_data.get('description', f\"A song in {STYLE_NAME} style\")\n",
    "print(f\"Generating with prompt: {prompt}\")\n",
    "\n",
    "gen_result = generator.generate(\n",
    "    melody_representation=rep_dict, \n",
    "    melody_audio_path=pre_meta.path, \n",
    "    style_name=STYLE_NAME,\n",
    "    prompt_text=prompt,\n",
    "    duration_sec=10 \n",
    ")\n",
    "\n",
    "gen_result.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d2d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_audio_path': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/final/gen_ambient_20251201_232336_final.wav',\n",
       " 'final_audio_path_mp3': None,\n",
       " 'duration_sec': 29.22,\n",
       " 'sample_rate': 16000,\n",
       " 'postprocessing_applied': ['normalize', 'fade_in', 'fade_out'],\n",
       " 'style_name': 'ambient',\n",
       " 'model_name': 'musicgen-transformers'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Post-processing & export\n",
    "postprocessor = Postprocessor()\n",
    "post_result = postprocessor.process(gen_result.audio_path, style_name=STYLE_NAME, model_name=gen_result.model_name)\n",
    "post_result_dict = post_result.to_dict()\n",
    "post_result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "631f571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Original: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav\n",
      "     vs Generated: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/final/gen_ambient_20251201_232336_final.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'style_name': 'ambient',\n",
       " 'model_name': 'musicgen-transformers',\n",
       " 'pitch_similarity': 0.8784857799686372,\n",
       " 'rhythm_similarity': 0.8784857799686372,\n",
       " 'overall_similarity': 0.8784857799686372,\n",
       " 'metadata': {'method': 'Audio Chroma DTW (Cosine)'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Similarity evaluation (original vs generated)\n",
    "\n",
    "evaluator = SimilarityEvaluator()\n",
    "\n",
    "print(f\"Comparing Original: {pre_meta.path}\")\n",
    "print(f\"     vs Generated: {post_result.final_audio_path}\")\n",
    "\n",
    "sim_report = evaluator.evaluate(\n",
    "    original_processed_audio=pre_meta.path,       \n",
    "    generated_audio=post_result.final_audio_path, \n",
    "    style_name=STYLE_NAME,\n",
    "    model_name=gen_result.model_name,\n",
    ")\n",
    "\n",
    "sim_report.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "habwt2jnv0o",
   "source": "# Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "44yz07r6v9u",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "# Create figure directory\n",
    "FIGURE_DIR = PROJECT_ROOT / \"latex\" / \"figures\"\n",
    "FIGURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load MIDI file and extract pitch information\n",
    "pm = pretty_midi.PrettyMIDI(contour.midi_path)\n",
    "\n",
    "# Extract notes from MIDI\n",
    "all_notes = []\n",
    "for instrument in pm.instruments:\n",
    "    if not instrument.is_drum:\n",
    "        for note in instrument.notes:\n",
    "            all_notes.append((note.start, note.end, note.pitch))\n",
    "\n",
    "all_notes.sort(key=lambda x: x[0])\n",
    "\n",
    "# Create time and pitch arrays from notes\n",
    "if all_notes:\n",
    "    time_arr = []\n",
    "    f0_midi_arr = []\n",
    "    for start, end, pitch in all_notes:\n",
    "        # Add points for each note\n",
    "        time_arr.extend([start, end])\n",
    "        f0_midi_arr.extend([pitch, pitch])\n",
    "    time_arr = np.array(time_arr)\n",
    "    f0_midi_arr = np.array(f0_midi_arr)\n",
    "else:\n",
    "    time_arr = np.array([0])\n",
    "    f0_midi_arr = np.array([60])\n",
    "\n",
    "# Plot pitch contour\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(time_arr, f0_midi_arr, 'b-', linewidth=2, label='Detected Notes')\n",
    "ax.scatter(time_arr[::2], f0_midi_arr[::2], c='blue', s=20, alpha=0.7, label='Note Onsets')\n",
    "\n",
    "ax.set_xlabel('Time (s)', fontsize=12)\n",
    "ax.set_ylabel('MIDI Note Number', fontsize=12)\n",
    "ax.set_title('Extracted Pitch Contour from Input Audio (via Basic Pitch MIDI)', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "if len(time_arr) > 0:\n",
    "    ax.set_xlim([0, max(time_arr) + 0.5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURE_DIR / 'pitch_contour.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(FIGURE_DIR / 'pitch_contour.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {FIGURE_DIR / 'pitch_contour.pdf'}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j8sl0sjcy3h",
   "source": [
    "import librosa.display\n",
    "\n",
    "# Load audio files\n",
    "y_input, sr_input = librosa.load(pre_meta.path, sr=None)\n",
    "y_output, sr_output = librosa.load(post_result.final_audio_path, sr=None)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=False)\n",
    "\n",
    "# Input waveform\n",
    "time_input = np.arange(len(y_input)) / sr_input\n",
    "axes[0].plot(time_input, y_input, color='steelblue', linewidth=0.5)\n",
    "axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0].set_title('(a) Preprocessed Input Audio Waveform', fontsize=12)\n",
    "axes[0].set_xlim([0, max(time_input)])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Output waveform\n",
    "time_output = np.arange(len(y_output)) / sr_output\n",
    "axes[1].plot(time_output, y_output, color='darkorange', linewidth=0.5)\n",
    "axes[1].set_xlabel('Time (s)', fontsize=11)\n",
    "axes[1].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[1].set_title(f'(b) Generated Output Audio Waveform (Style: {post_result.style_name})', fontsize=12)\n",
    "axes[1].set_xlim([0, max(time_output)])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURE_DIR / 'waveform_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(FIGURE_DIR / 'waveform_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {FIGURE_DIR / 'waveform_comparison.pdf'}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "ftjye4eznfm",
   "source": [
    "all_styles = style_manager.list_styles()\n",
    "print(f\"Available styles: {all_styles}\")\n",
    "\n",
    "similarity_results = []\n",
    "\n",
    "for style_name in all_styles:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing style: {style_name}\")\n",
    "    \n",
    "    # Generate\n",
    "    gen_res = generator.generate(melody_representation=rep_dict, style_name=style_name)\n",
    "    print(f\"  Generated: {gen_res.audio_path}\")\n",
    "    \n",
    "    # Post-process\n",
    "    post_res = postprocessor.process(gen_res.audio_path, style_name=style_name, model_name=gen_res.model_name)\n",
    "    print(f\"  Post-processed: {post_res.final_audio_path}\")\n",
    "    \n",
    "    # Evaluate similarity\n",
    "    sim_rep = evaluator.evaluate(\n",
    "        original_processed_audio=pre_meta.path,\n",
    "        generated_audio=post_res.final_audio_path,\n",
    "        style_name=style_name,\n",
    "        model_name=gen_res.model_name,\n",
    "    )\n",
    "    \n",
    "    similarity_results.append({\n",
    "        'style': style_name,\n",
    "        'pitch_sim': sim_rep.pitch_similarity,\n",
    "        'rhythm_sim': sim_rep.rhythm_similarity,\n",
    "        'overall': sim_rep.overall_similarity,\n",
    "    })\n",
    "    print(f\"  Pitch Similarity: {sim_rep.pitch_similarity:.3f}\")\n",
    "    print(f\"  Rhythm Similarity: {sim_rep.rhythm_similarity:.3f}\")\n",
    "    print(f\"  Overall Similarity: {sim_rep.overall_similarity:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"All styles processed!\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "j8umtj3yyll",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(similarity_results)\n",
    "df.columns = ['Style', 'Pitch Similarity', 'Rhythm Similarity', 'Overall Score']\n",
    "\n",
    "# Capitalize style names for display\n",
    "df['Style'] = df['Style'].str.capitalize()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMILARITY EVALUATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Generate LaTeX table code\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LATEX TABLE CODE (copy this to main.tex)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "latex_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    style = row['Style']\n",
    "    if style == '8bit':\n",
    "        style = '8-bit'\n",
    "    latex_rows.append(f\"{style} & {row['Pitch Similarity']:.2f} & {row['Rhythm Similarity']:.2f} & {row['Overall Score']:.2f} \\\\\\\\\")\n",
    "\n",
    "print(\"\"\"\\\\begin{table}[H]\n",
    "\\\\centering\n",
    "\\\\caption{Similarity evaluation results across musical styles (stub generator baseline)}\n",
    "\\\\label{tab:similarity}\n",
    "\\\\begin{tabular}{lccc}\n",
    "\\\\toprule\n",
    "Style & Pitch Similarity & Rhythm Similarity & Overall Score \\\\\\\\\n",
    "\\\\midrule\"\"\")\n",
    "for row in latex_rows:\n",
    "    print(row)\n",
    "print(\"\"\"\\\\bottomrule\n",
    "\\\\end{tabular}\n",
    "\\\\end{table}\"\"\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wdgoikvb39",
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# (a) Input waveform\n",
    "ax = axes[0, 0]\n",
    "y_in, sr_in = librosa.load(pre_meta.path, sr=None)\n",
    "t_in = np.arange(len(y_in)) / sr_in\n",
    "ax.plot(t_in, y_in, color='steelblue', linewidth=0.5)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('(a) Preprocessed Input Waveform')\n",
    "ax.set_xlim([0, max(t_in)])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (b) Pitch contour from MIDI\n",
    "ax = axes[0, 1]\n",
    "pm_viz = pretty_midi.PrettyMIDI(contour.midi_path)\n",
    "viz_notes = []\n",
    "for inst in pm_viz.instruments:\n",
    "    if not inst.is_drum:\n",
    "        for note in inst.notes:\n",
    "            viz_notes.append((note.start, note.end, note.pitch))\n",
    "viz_notes.sort(key=lambda x: x[0])\n",
    "\n",
    "if viz_notes:\n",
    "    for start, end, pitch in viz_notes:\n",
    "        ax.plot([start, end], [pitch, pitch], 'b-', linewidth=3, alpha=0.7)\n",
    "        ax.scatter([start], [pitch], c='blue', s=30, zorder=5)\n",
    "    max_time = max(n[1] for n in viz_notes)\n",
    "else:\n",
    "    max_time = 1.0\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('MIDI Note Number')\n",
    "ax.set_title('(b) Extracted Pitch Contour (from MIDI)')\n",
    "ax.set_xlim([0, max_time + 0.5])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (c) Note segmentation\n",
    "ax = axes[1, 0]\n",
    "notes = rep.note_sequence\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(notes) if notes else 1))\n",
    "for i, note in enumerate(notes):\n",
    "    ax.barh(note.pitch_midi, note.duration, left=note.start, \n",
    "            height=0.8, color=colors[i % len(colors)], edgecolor='black', linewidth=0.5)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('MIDI Note Number')\n",
    "ax.set_title(f'(c) Note Segmentation ({len(notes)} notes)')\n",
    "if notes:\n",
    "    ax.set_xlim([0, max(n.start + n.duration for n in notes) + 0.5])\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# (d) Generated output waveform\n",
    "ax = axes[1, 1]\n",
    "y_out, sr_out = librosa.load(post_result.final_audio_path, sr=None)\n",
    "t_out = np.arange(len(y_out)) / sr_out\n",
    "ax.plot(t_out, y_out, color='darkorange', linewidth=0.5)\n",
    "ax.set_xlabel('Time (s)')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title(f'(d) Generated Output Waveform (Style: {post_result.style_name})')\n",
    "ax.set_xlim([0, max(t_out)])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURE_DIR / 'pipeline_demo.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(FIGURE_DIR / 'pipeline_demo.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved to {FIGURE_DIR / 'pipeline_demo.pdf'}\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
