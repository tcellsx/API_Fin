{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "47434634",
      "metadata": {},
      "source": [
        "# humming2music Demo Pipeline\n",
        "Linear notebook that stitches modules 01-09 together.\n",
        "\n",
        "Adjust the variables in each cell to run the full pipeline. Dependencies: pydub, librosa, numpy, sounddevice (optional for recording), ffmpeg for pydub/mp3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "37a11aa5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python exe: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/bin/python\n",
            "Python version: 3.10.15\n",
            "Using interpreter: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/bin/python\n",
            "Requirement already satisfied: librosa in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (0.11.0)\n",
            "Requirement already satisfied: pydub in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (0.25.1)\n",
            "Requirement already satisfied: soundfile in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (0.13.1)\n",
            "Requirement already satisfied: sounddevice in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (0.5.3)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (0.62.1)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.7.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (5.2.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "print(\"Python exe:\", sys.executable)\n",
        "print(\"Python version:\", platform.python_version())\n",
        "\n",
        "import sys\n",
        "print(\"Using interpreter:\", sys.executable)\n",
        "\n",
        "# Âú®ÂΩìÂâçÂÜÖÊ†∏ÂØπÂ∫îÁöÑ python ÈáåË£ÖÂ∫ì\n",
        "import subprocess\n",
        "subprocess.check_call([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\",\n",
        "    \"librosa\", \"pydub\", \"soundfile\", \"sounddevice\",\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9da227ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "librosa: 0.11.0\n",
            "pydub: 0.25.1\n"
          ]
        }
      ],
      "source": [
        "import importlib.metadata as md\n",
        "\n",
        "print(\"librosa:\", md.version(\"librosa\"))\n",
        "print(\"pydub:\", md.version(\"pydub\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cfed8b69",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python exe: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/bin/python\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "scikit-learn version 1.7.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.5.1. Disabling scikit-learn conversion API.\n",
            "Torch version 2.9.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.7.0 is the most recent version that has been tested.\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'Could not load this library: /opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torchvision/image.so'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "WARNING:root:tflite-runtime is not installed. If you plan to use a TFLite Model, reinstall basic-pitch with `pip install 'basic-pitch tflite-runtime'` or `pip install 'basic-pitch[tf]'\n",
            "WARNING:root:Tensorflow is not installed. If you plan to use a TF Saved Model, reinstall basic-pitch with `pip install 'basic-pitch[tf]'`\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/resampy/filters.py:50: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music\n",
            "Data/raw: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/raw\n",
            "Outputs/generated: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/generated\n"
          ]
        }
      ],
      "source": [
        "# 0. Setup paths and imports\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import librosa, pydub\n",
        "\n",
        "print(\"Python exe:\", sys.executable)\n",
        "\n",
        "PROJECT_ROOT = Path('..').resolve()\n",
        "sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from src.config import (\n",
        "    GLOBAL_AUDIO_CONFIG,\n",
        "    DEFAULT_PREPROCESSING_CONFIG,\n",
        "    DEFAULT_MELODY_EXTRACTION_CONFIG,\n",
        "    DEFAULT_MELODY_REPRESENTATION_CONFIG,\n",
        "    GLOBAL_STYLE_CONFIG,\n",
        "    DEFAULT_POSTPROCESSING_CONFIG,\n",
        "    DEFAULT_SIMILARITY_CONFIG,\n",
        "    RAW_AUDIO_DIR, PROCESSED_AUDIO_DIR, GENERATED_AUDIO_DIR, POSTPROCESSED_AUDIO_DIR, EVAL_OUTPUT_DIR,\n",
        ")\n",
        "\n",
        "from src.audio_input import AudioInputManager\n",
        "from src.preprocessing import Preprocessor\n",
        "from src.melody_extraction import MelodyExtractor\n",
        "from src.melody_representation import MelodyRepresenter\n",
        "from src.style_and_model_config import StyleConfigManager\n",
        "from src.music_generation import MusicGenerator\n",
        "from src.postprocessing_export import Postprocessor\n",
        "from src.similarity_evaluation import SimilarityEvaluator\n",
        "\n",
        "for d in [RAW_AUDIO_DIR, PROCESSED_AUDIO_DIR, GENERATED_AUDIO_DIR, POSTPROCESSED_AUDIO_DIR, EVAL_OUTPUT_DIR]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Project root:', PROJECT_ROOT)\n",
        "print('Data/raw:', RAW_AUDIO_DIR)\n",
        "print('Outputs/generated:', GENERATED_AUDIO_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1ff89cff",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/raw/raw_input_20251201_231802_sessiondemo_sine.wav',\n",
              " 'duration_sec': 19.722,\n",
              " 'sample_rate': 16000,\n",
              " 'channels': 1,\n",
              " 'format': 'wav',\n",
              " 'source_type': 'uploaded'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Audio input (upload or record)\n",
        "from pathlib import Path\n",
        "\n",
        "SESSION_ID = \"demo_sine\"\n",
        "\n",
        "# Áî®ÁîüÊàêÁöÑÊµãËØïÊñá‰ª∂\n",
        "UPLOAD_PATH = \"/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/raw/womanhumming.mp3\"\n",
        "\n",
        "audio_manager = AudioInputManager()\n",
        "audio_meta = audio_manager.ingest_upload(UPLOAD_PATH, session_id=SESSION_ID)\n",
        "\n",
        "audio_meta_dict = audio_meta.to_dict()\n",
        "audio_meta_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d523282",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav',\n",
              " 'original_duration_sec': 19.722,\n",
              " 'processed_duration_sec': 18.415,\n",
              " 'sample_rate': 16000,\n",
              " 'applied_steps': ['trim_silence', 'highpass', 'normalize'],\n",
              " 'notes': ''}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Preprocessing\n",
        "preprocessor = Preprocessor()\n",
        "pre_meta = preprocessor.preprocess(audio_meta.path)\n",
        "pre_meta_dict = pre_meta.to_dict()\n",
        "pre_meta_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e9d112fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicting MIDI for /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav...\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "isfinite: True\n",
            "shape: (1, 43844, 1)\n",
            "dtype: float32\n",
            "\n",
            "\n",
            "  Creating midi...\n",
            "  üíÖ Saved to /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine_basic_pitch.mid\n",
            "Generated MIDI: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine_basic_pitch.mid\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'extractor': 'basic_pitch (onnx)',\n",
              " 'source_audio': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Melody extraction\n",
        "extractor = MelodyExtractor()\n",
        "contour = extractor.extract(pre_meta.path)\n",
        "contour_dict = contour.to_dict()\n",
        "print(f\"Generated MIDI: {contour_dict['midi_path']}\")\n",
        "contour_dict['metadata']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c1c5016b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'notes': 52, 'tempo_bpm': 183.91386953768213}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4. Melody representation\n",
        "representer = MelodyRepresenter()\n",
        "\n",
        "rep = representer.represent(midi_path=contour.midi_path)\n",
        "rep_dict = rep.to_dict()\n",
        "\n",
        "rep_summary = {\n",
        "    'notes': len(rep_dict['note_sequence']),\n",
        "    'tempo_bpm': rep_dict['rhythm_profile'].get('estimated_tempo_bpm'),\n",
        "}\n",
        "rep_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0c8fc62b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'ambient',\n",
              " 'description': 'Ethereal ambient soundscape with pads and evolving textures.',\n",
              " 'mood': 'calm',\n",
              " 'tempo_bpm': 70,\n",
              " 'instruments': ['pads', 'drones', 'textures'],\n",
              " 'model_configs': {'stub': {'model_name': 'stub',\n",
              "   'prompt': 'A calm ambient soundscape with airy pads and evolving textures.',\n",
              "   'max_duration_sec': 30}}}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 5. Style selection\n",
        "style_manager = StyleConfigManager()\n",
        "available_styles = style_manager.list_styles()\n",
        "STYLE_NAME = available_styles[1]  # pick first by default\n",
        "style_config = style_manager.get_style(STYLE_NAME)\n",
        "style_config.to_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a40e2f2e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type musicgen_melody to instantiate a model of type musicgen. This is not supported for all configurations of models and can yield errors.\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/music_gen/lib/python3.10/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:11<00:00,  5.58s/it]\n",
            "Some weights of MusicgenForConditionalGeneration were not initialized from the model checkpoint at facebook/musicgen-melody and are newly initialized: ['decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.12.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.12.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.13.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.13.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.14.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.14.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.15.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.15.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.16.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.16.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.17.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.17.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.18.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.18.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.19.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.19.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.20.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.20.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.21.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.21.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.22.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.22.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.23.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.23.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.24.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.24.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.24.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.25.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.25.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.25.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.26.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.26.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.26.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.27.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.27.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.27.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.28.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.28.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.28.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.29.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.29.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.29.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.30.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.30.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.30.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.31.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.31.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.31.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.32.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.32.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.32.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.33.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.33.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.33.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.34.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.34.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.34.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.35.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.35.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.35.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.36.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.36.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.36.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.37.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.37.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.37.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.38.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.38.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.38.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.39.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.39.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.39.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.40.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.40.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.40.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.41.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.41.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.41.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.42.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.42.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.42.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.43.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.43.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.43.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.44.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.44.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.44.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.45.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.45.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.45.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.46.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.46.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.46.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.47.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.47.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.47.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating with prompt: Ethereal ambient soundscape with pads and evolving textures.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'model_name': 'musicgen-transformers',\n",
              " 'style_name': 'ambient',\n",
              " 'audio_path': 'outputs/generated/gen_ambient_20251201_232336.wav',\n",
              " 'duration_sec': 10,\n",
              " 'generation_metadata': {'prompt': 'Ethereal ambient soundscape with pads and evolving textures.'}}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 6. Music generation \n",
        "\n",
        "generator = MusicGenerator(model_size='melody', device='cpu') \n",
        "\n",
        "if hasattr(style_config, 'to_dict'):\n",
        "    style_data = style_config.to_dict()\n",
        "else:\n",
        "    style_data = style_config\n",
        "\n",
        "prompt = style_data.get('description', f\"A song in {STYLE_NAME} style\")\n",
        "print(f\"Generating with prompt: {prompt}\")\n",
        "\n",
        "gen_result = generator.generate(\n",
        "    melody_representation=rep_dict, \n",
        "    melody_audio_path=pre_meta.path, \n",
        "    style_name=STYLE_NAME,\n",
        "    prompt_text=prompt,\n",
        "    duration_sec=10 \n",
        ")\n",
        "\n",
        "gen_result.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c9d2d13e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'final_audio_path': '/Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/final/gen_ambient_20251201_232336_final.wav',\n",
              " 'final_audio_path_mp3': None,\n",
              " 'duration_sec': 29.22,\n",
              " 'sample_rate': 16000,\n",
              " 'postprocessing_applied': ['normalize', 'fade_in', 'fade_out'],\n",
              " 'style_name': 'ambient',\n",
              " 'model_name': 'musicgen-transformers'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 7. Post-processing & export\n",
        "postprocessor = Postprocessor()\n",
        "post_result = postprocessor.process(gen_result.audio_path, style_name=STYLE_NAME, model_name=gen_result.model_name)\n",
        "post_result_dict = post_result.to_dict()\n",
        "post_result_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "631f571a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparing Original: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/data/processed/processed_input_20251201_231802_sessiondemo_sine.wav\n",
            "     vs Generated: /Users/xijiecao/Desktop/course/s3/API/API_Fin-main/humming2music/outputs/final/gen_ambient_20251201_232336_final.wav\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'style_name': 'ambient',\n",
              " 'model_name': 'musicgen-transformers',\n",
              " 'pitch_similarity': 0.8784857799686372,\n",
              " 'rhythm_similarity': 0.8784857799686372,\n",
              " 'overall_similarity': 0.8784857799686372,\n",
              " 'metadata': {'method': 'Audio Chroma DTW (Cosine)'}}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 8. Similarity evaluation (original vs generated)\n",
        "\n",
        "evaluator = SimilarityEvaluator()\n",
        "\n",
        "print(f\"Comparing Original: {pre_meta.path}\")\n",
        "print(f\"     vs Generated: {post_result.final_audio_path}\")\n",
        "\n",
        "sim_report = evaluator.evaluate(\n",
        "    original_processed_audio=pre_meta.path,       \n",
        "    generated_audio=post_result.final_audio_path, \n",
        "    style_name=STYLE_NAME,\n",
        "    model_name=gen_result.model_name,\n",
        ")\n",
        "\n",
        "sim_report.to_dict()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "music_gen",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
